{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b54d4462-0bc2-409a-926d-0daaab06f548",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Take home assessment for Spotter's Sr. Data Engineer role\n",
    "\n",
    "At Spotter we work with a lot of YouTube data, we have prepared a small sample dataset for you to use during this assignment.\n",
    "While this is a tiny dataset provided as a sample for you to work with, your submission needs to keep the scaleability in mind. Our datasets have many millions of records, sometimes billions of records so your solution should work regardless of the scale of the input in a performant manner.\n",
    "\n",
    "The first cell includes the paths to the datasets that are stored in parquet and some helper functions that you would use to get video level view count for a batch of videos in a single call.\n",
    "Note that we will not use the live YouTube APIs in this excersise and instead will assign a number per video_id to represent the views as of the time of the call (lifetime views)\n",
    "\n",
    "There are three problems, please complete as many as you are able.\n",
    "* Calculate monthly views from daily data: Using the daily level data in the sample dataset, count monthly views per video by aggregating the daily `viewCount`\n",
    "* In some cases (for varying reasons) we have gaps in the data. Using the sample dataset find and present the date range gaps per video. The dataset has daily level records for selected videos from the date of video upload (created_date) till 2023-04-18.\n",
    "* With some YouTube APIs we are able to send a batch of video_ids to get metrics such as lifetime views. For this problem, use the first two columns (`channel_id` and `video_id`) from the sample dataset and add two more columns (`date` with the value of the run date and `viewCount` that you can get from the provided helper function) \n",
    "\n",
    "Note: The default notebook language is set to Python but you can change and complete it using Spark's Scala APIs if you are more comfortable with it. We are not looking for SQL based solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a272e2f0-2ee4-4a75-9466-233a4ed610e4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StructType, StructField, StringType, IntegerType\n",
    "\n",
    "path_video_samples = \"s3://dataeng-interview-datasets/videos_sample_m\" # parquet dataset\n",
    "\n",
    "\n",
    "## Helper code for Problem 3 ##\n",
    "\n",
    "# schema for the return type of process_batch_of_videos\n",
    "views_schema = ArrayType(\n",
    "    StructType([\n",
    "        StructField(\"video_id\", StringType(), False),\n",
    "        StructField(\"views\", IntegerType(), False)\n",
    "    ])\n",
    ")\n",
    "\n",
    "# YouTube API response simulator on a single video\n",
    "def calculate_views(video_id):\n",
    "    # for this excersise we'll just generate a number instead of using the API\n",
    "    return int(abs(hash(video_id) % 1000))\n",
    "\n",
    "# UDF helper function that takes a list of video_ids (as a list) and returns a list of tuples\n",
    "# :param video_ids: a list of up to 10 video_id values\n",
    "# :return a list of tuples (video_id, viewCount). Same size as the input\n",
    "@udf(returnType=views_schema)\n",
    "def process_batch_of_videos(video_ids):\n",
    "    if len(video_ids) > 10:\n",
    "        raise Exception(f\"Too many ({len(video_ids)}) videos in the list. Expected up to 10\")\n",
    "    return [(vid, calculate_views(vid)) for vid in video_ids]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64d3dd1e-aea6-4924-8f1c-8d0a195a3a2a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Load the data\n",
    "First, let's create a dataframe with the sample data and display it in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2cf7aa5-a9d6-487b-bebe-824fe2760eea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# initialize a Spark dataframe\n",
    "df_samples = spark.read.parquet(path_video_samples)\n",
    "display(df_samples) # display it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "741a71b7-54fb-45e2-93c0-86d183271ca0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Let's set some configs used to improve performance\n",
    "##### DBR 15.4 - This is the runtime used with this notebook. \n",
    "**AQE** - Adaptive Query Execution dynamically optimizes query plans based on runtime statistics, which helps improve performance by adjusting things like join strategies, reducing the number of partitions, and handling skewed data more effectively.\n",
    "\n",
    "**IO Caching** - Enable Databricks I/O cache for faster data retrieval by caching input data on local SSDs\n",
    "\n",
    "**maxPartitionBytes** - Set the maximum size of files to read per partition, optimizing parallelism and memory usage\n",
    "\n",
    "**CBO** - Enable Cost-Based Optimization (CBO) to improve query performance by using table statistics for query planning\n",
    "\n",
    "**autoBroadcastJoinThreshold** - Set the size threshold for automatically broadcasting tables in joins to avoid shuffling\n",
    "\n",
    "**shuffle.partitions** - Set the default number of partitions for shuffle operations, impacting the parallelism of jobs.  Note that AQE will dynamically adjust this value at runtime but a good initial value is recommended. \n",
    "\n",
    "*These are just suggestions of things that can be used to improve performance.   Note that some of them take contol away from the developer.   A good understanding of each config and it's behavior on your job is recommended.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bf72edc-6782-41ee-b107-e30e2128b0ce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Enable Adaptive Query Execution (AQE) to dynamically optimize query plans at runtime\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "\n",
    "# Enable skew join optimization, which splits skewed partitions during joins for better load balancing\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "\n",
    "# Enable dynamic coalescing of shuffle partitions, which merges small partitions into larger ones for better parallelism and reduced task overhead\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ee4166b-904a-4676-b372-17f699744e95",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Enable Databricks I/O cache to speed up repeated file reads from S3 by caching frequently accessed data in memory and local SSDs.  \n",
    "# This reduces the need to re-read data from S3, which improves performance for iterative queries and workloads.\n",
    "spark.conf.set(\"spark.databricks.io.cache.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "556c5e38-7b7b-47db-b3ee-893cbe7afcb3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set maxPartitionBytes to 128MB = 128 * 1024 * 1024\n",
    "spark.conf.set('spark.sql.files.maxPartitionBytes', \"134217728b\")   # Note this is the default\n",
    "\n",
    "# Used to control the maximum size of data partitions when Spark reads data from a file source (such as Parquet, ORC, JSON or CSV). It helps determine the size of splits, spark will use when reading the input files.  This ensures that the data is divided into manageable partition sizes for efficient parallel processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4002c0bb-843d-4e9a-9c00-8f67c618bbe6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Enable Cost-Based Optimization (CBO)\n",
    "spark.conf.set(\"spark.sql.cbo.enabled\", \"true\")\n",
    "\n",
    "# Enable join reordering based on collected statistics\n",
    "spark.conf.set(\"spark.sql.cbo.joinReorder.enabled\", \"true\")\n",
    "\n",
    "# Enable creation of histograms to improve CBO decision-making for skewed data\n",
    "spark.conf.set(\"spark.sql.statistics.histogram.enabled\", \"true\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f719935-077c-41e0-b998-7d3e68f70f7e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set the broadcast join threshold as needed based on the size of the \"smaller\" dataset.  Adjust the cluster size to have more executor memory to handle the broadcast.  \n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"10MB\")  # Default is 10 MB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0a38925-7352-4b66-bed6-b4ee51dfa220",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'200'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set an appropriate value for \n",
    "# spark.sql.shuffle.partitions.  \n",
    "# This value defaults to 200.  \n",
    "# The formula below is a good place to start. \n",
    "# 128 MB - 1 GB is the recommended target size range. \n",
    "\n",
    "# optimal_partitions = largest_total_shuffle_size / target_partition_size\n",
    "\n",
    "# optimal_partitions = largest_total_shuffle_size MB / 128 MB\n",
    "\n",
    "#spark.conf.set(\"spark.sql.shuffle.partitions\", 200)\n",
    "spark.conf.get(\"spark.sql.shuffle.partitions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "630e60ca-2b1a-4664-8d7d-1fef7764d603",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Problem 1\n",
    "Using the sample dataset, create a new dataframe with the following columns: `channel_id`, `video_id`, `month` and `aggViewCount` where:\n",
    "* `channel_id` and `video_id` values are from the sample dataset\n",
    "* `month` has a format of YYYY-MM to represent the month for which the aggregated view count was computed\n",
    "* `aggViewCount` is the sum of views for a given `channel_id` and `video_id` pair over all days of the month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74d66cf4-b8a0-4e37-b491-ee3311dbcacd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Problem 1:\n",
    "\n",
    "from pyspark.sql.functions import date_format, sum, col\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "# Load only the necessary columns from the columnar source\n",
    "df_samples = spark.read.parquet(path_video_samples).select(\"channel_id\", \"video_id\", \"date\", \"viewCount\")\n",
    "\n",
    "# Add a column 'month' in the format YYYY-MM based on the 'date' column\n",
    "df_samples_with_month = df_samples.withColumn(\"month\", date_format(col(\"date\"), \"yyyy-MM\"))\n",
    "\n",
    "# Persist after adding the month column since it will be used twice in the aggregation\n",
    "# Use memory and disk in production to give the job a better chance to succeed\n",
    "# If you see disk used, add more memory to your cluster\n",
    "df_samples_with_month.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "# display(df_samples_with_month)\n",
    "\n",
    "# Perform the aggregation\n",
    "df_monthly_views = df_samples_with_month.groupBy(\"channel_id\", \"video_id\", \"month\") \\\n",
    "    .agg(sum(\"viewCount\").alias(\"aggViewCount\"))\n",
    "\n",
    "# Display the final aggregated result\n",
    "display(df_monthly_views)\n",
    "\n",
    "# Unpersist cached DataFrame since no longer needed\n",
    "df_samples_with_month.unpersist()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2680fbfb-6f45-42ba-a1fe-95e211a9c1f3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Problem 2\n",
    "If all goes well with our data pulls we anticipate to have full daily level data for all videos starting from the upload date. However, sometimes we get no response from YouTube APIs (it is a result of some backend issue at their side). We can deal with these gaps if we can identify the dates that we don't have any data for a given video.\n",
    "\n",
    "Let's write Spark code to find all missing dates. Remember that our sample data should have daily level viewCount up to 2023-04-18\n",
    "\n",
    "Create a new dataframe with the following columns: `channel_id`, `video_id`, `created_date` and `missing_dates` (or `missing_date`) where:\n",
    "* `channel_id`, `video_id` and `created_date` values are from the sample dataset\n",
    "* `missing_dates` is a list of dates for which a given video does not have data\n",
    "* `missing_date` is a single date value (in this case we'll have one record per video) that is missing from available dates\n",
    "\n",
    "Only keep the records that have missing dates\n",
    "\n",
    "**Bonus Point** if you create both datasets one with `missing_dates` and the other with `missing_date` schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4b2ff02-3785-4e8a-bd17-71eee3255173",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Problem 2:\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Load the sample data (selecting only necessary columns)\n",
    "df_samples = spark.read \\\n",
    "    .parquet(path_video_samples) \\\n",
    "    .select(\"channel_id\", \"video_id\", \"date\", \"created_date\")\n",
    "#df_samples.show(10, truncate=False)\n",
    "#print(f\"df_sample.count = {df_samples.count()}\")\n",
    "\n",
    "\n",
    "# Get distinct video records (channel_id, video_id, created_date)\n",
    "df_dates_distinct = df_samples.select(\"channel_id\", \"video_id\", \"created_date\").distinct()\n",
    "# df_samples = df_samples.withColumnRenamed(\"date\", \"unique_created_date\")\n",
    "# df_dates_distinct = df_samples.dropDuplicates([\"channel_id\", \"video_id\", \"unique_created_date\"])\n",
    "# df_samples = df_samples.withColumnRenamed(\"unique_created_date\", \"date\")\n",
    "\n",
    "#df_dates_distinct.show(10, truncate=False)\n",
    "#print(f\"df_dates_distinct.count = {df_dates_distinct.count()}\")\n",
    "\n",
    "# Cache the date range to avoid recalculating the sequence\n",
    "df_dates_distinct.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "\n",
    "# Create the sequence of dates as an array in df_dates_with_sequence\n",
    "df_dates_with_sequence = df_dates_distinct.withColumn(\n",
    "    \"date_range\",\n",
    "    F.expr(f\"sequence(created_date, to_date('2023-04-18'), interval 1 day)\")\n",
    ")\n",
    "# display(df_dates_with_sequence)\n",
    "# print(f\"df_dates_with_sequence = {df_dates_with_sequence.count()}\")\n",
    "\n",
    "\n",
    "# Join df_dates_with_sequence with df_samples on video_id to get the actual dates\n",
    "df_combined = df_dates_with_sequence.join(\n",
    "    df_samples.select(\"video_id\", \"date\"), \n",
    "    on=\"video_id\", \n",
    "    how=\"left\"\n",
    ").dropDuplicates([\"video_id\", \"channel_id\", \"created_date\", \"date_range\"])\n",
    "# display(df_combined)\n",
    "# print(f\"df_combined = {df_combined.count()}\")\n",
    "\n",
    "\n",
    "# Use collect_list to gather the actual dates for each video_id and compare using array_except\n",
    "df_missing_dates = df_combined.withColumn(\n",
    "    \"missing_dates\", \n",
    "    F.expr(\"array_except(date_range, collect_list(date) over (partition by video_id))\")\n",
    ").select(\"channel_id\", \"video_id\", \"created_date\", \"missing_dates\")\n",
    "\n",
    "display(df_missing_dates)\n",
    "# print(f\"df_missing_dates = {df_missing_dates.count()}\")\n",
    "\n",
    "\n",
    "# Optionally explode missing_dates to get rows for each missing date\n",
    "df_missing_date = df_missing_dates.withColumn(\n",
    "    \"missing_date\", \n",
    "    F.explode(\"missing_dates\")\n",
    ").select(\"channel_id\", \"video_id\", \"created_date\", \"missing_date\")\n",
    "\n",
    "display(df_missing_date)\n",
    "# print(f\"df_missing_date = {df_missing_date.count()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e28bcc0-1afd-40ec-884a-ef9daf016e8f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Problem 3\n",
    "In most cases we are able to batch multiple `video_id`s in a single YouTube API call and get a response for the full batch.\n",
    "\n",
    "In simplified form it means getting back an array of tuples (`video_id`, `viewCount`) for an input of list of `video_id`s.\n",
    "\n",
    "With the provided helper function `process_batch_of_videos` we can simulate this behavior and get the lifetime view count as of the execution time. The functions accepts a list of up to 10 videos as an input.\n",
    "\n",
    "Let's write Spark code to get a `viewCount` value per unique video in the sample dataset and add these records to create a new dataset with lifetime views as of today.\n",
    "\n",
    "There are two expected dataframes, one with only the views as of today and the second one is to combine the first with historical data that will give us view count snapshot for all of the days we have a knowledge of. \n",
    "\n",
    "Note: Due to the way we generated the view counts for this exercise, the count would not be increasing for newer dates but we don't need to worry about it for the purposes of this assessment.\n",
    "\n",
    "Create a new dataframe with the following columns: `channel_id`, `video_id`, `created_date`, `date` and `viewCount` where:\n",
    "* `channel_id`, `video_id` and `created_date` values are from the sample dataset\n",
    "* `date` is the date you ran the code when you got the `viewCount` from the provided function\n",
    "* `viewCount` from the provided function for the `video_id`\n",
    "\n",
    "The second dataset has the same schema as the first one only it has the combined data from before and first dataset in this problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e41d0c00-a52b-4003-9714-99a70da77205",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Problem 3:\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import date\n",
    "\n",
    "\n",
    "# Load the sample dataset and partition it by channel_id and created_date for more efficient reads\n",
    "df_samples = spark.read.parquet(path_video_samples) \\\n",
    "    .select(\"channel_id\", \"video_id\", \"created_date\", \"viewCount\") #\\\n",
    "    # .repartition(\"channel_id\", \"created_date\")\n",
    "\n",
    "\n",
    "# Add today's date (execution date) as 'date'\n",
    "today_date = date.today()\n",
    "df_samples_with_date = df_samples.withColumn(\"date\", F.lit(today_date))\n",
    "\n",
    "# Group video_ids into a list per channel_id and created_date\n",
    "df_grouped_videos = df_samples_with_date.groupBy(\"channel_id\", \"created_date\", \"date\").agg(\n",
    "    F.collect_list(\"video_id\").alias(\"video_ids\")\n",
    ")\n",
    "\n",
    "# Calculate the number of batches (size of video_ids divided by batch size)\n",
    "batch_size = 10\n",
    "df_grouped_videos = df_grouped_videos.withColumn(\"video_count\", F.size(\"video_ids\"))\n",
    "df_grouped_videos = df_grouped_videos.withColumn(\"num_batches\", F.expr(f\"ceil(video_count / {batch_size})\"))\n",
    "\n",
    "# Generate sequence of batch numbers for splitting\n",
    "df_with_batches = df_grouped_videos.withColumn(\"batch_numbers\", F.expr(f\"sequence(1, num_batches)\"))\n",
    "\n",
    "# Explode batch numbers to create multiple rows, each representing a batch\n",
    "df_exploded_batches = df_with_batches.withColumn(\"batch_number\", F.explode(\"batch_numbers\"))\n",
    "\n",
    "# Use the batch_number to slice the video_ids into batches of size 10\n",
    "df_batched_videos = df_exploded_batches.withColumn(\n",
    "    \"video_batch\", \n",
    "    F.expr(f\"slice(video_ids, (batch_number - 1) * {batch_size} + 1, {batch_size})\")\n",
    ").drop(\"video_count\", \"num_batches\", \"batch_numbers\", \"batch_number\")\n",
    "\n",
    "# Cache the DataFrame with batched videos to avoid recomputation in case of reuse\n",
    "df_batched_videos.cache()\n",
    "\n",
    "# Apply the process_batch_of_videos UDF to get view counts for each batch\n",
    "df_with_views = df_batched_videos.withColumn(\n",
    "    \"view_data\", process_batch_of_videos(F.col(\"video_batch\"))\n",
    ")\n",
    "\n",
    "# Explode the result to get individual video_id and viewCount pairs\n",
    "df_exploded = df_with_views.withColumn(\"exploded_view\", F.explode(\"view_data\"))\n",
    "\n",
    "# Select individual columns from the exploded struct and alias them correctly\n",
    "df_lifetime_views = df_exploded.select(\n",
    "    \"channel_id\", \n",
    "    \"created_date\", \n",
    "    \"date\", \n",
    "    F.col(\"exploded_view.video_id\").alias(\"video_id\"), \n",
    "    F.col(\"exploded_view.views\").alias(\"viewCount\")\n",
    ")\n",
    "\n",
    "# Remove duplicate rows\n",
    "df_lifetime_views = df_lifetime_views.dropDuplicates([\"channel_id\", \"video_id\", \"created_date\", \"date\"])\n",
    "#df_lifetime_views = df_lifetime_views.select(\"channel_id\", \"video_id\", \"created_date\", \"date\").distinct()\n",
    "\n",
    "\n",
    "# Display the dataframe with lifetime views (today's view counts), ordered by viewCount\n",
    "display(df_lifetime_views)\n",
    "\n",
    "# Combine lifetime views with historical data (ensure column names match)\n",
    "df_combined = df_samples_with_date.unionByName(df_lifetime_views).select(\"channel_id\", \"created_date\", \"date\", \"video_id\", \"viewCount\")\n",
    "\n",
    "# Display the combined dataframe, ordered by viewCount\n",
    "display(df_combined)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4046298703544804,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Sr DE Assessment",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
